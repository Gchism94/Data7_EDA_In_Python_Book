[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data 7 Exploratory Data Analysis In Python Book",
    "section": "",
    "text": "Here you will find a collection of workshops prepared by the staff of the Data Science Institute\n\n\n\n\n\nExploring a novel data set and produce an HTML interactive reports\n\n\n\n\n\n\n\nExploring the normality of numerical columns in a novel data set\n\n\n\n\n\n\n\nUsing data transformation to correct non-normality in numerical data\n\n\n\n\n\n\n\nExploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set\n\n\n\n\n\n\n\nAssess relationships within a novel data set\n\nVisit our available Digital Learning Resources Library!\n\nCreated: 09/14/2022 (G. Chism); Last update: 09/14/2022\n CC BY-NC-SA"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Exploratory data analysis is an essential first step towards determining the validity of your data and should be performed throughout the data pipeline. However, EDA is often performed too late or not at all. The Python programming language, is a widely used open source platform for data analysis and data visualization. This is because of the variety of libraries available and attentive community devoted to data analysis.\nHere, we utilize the pandas and pandas-profiling libraries to conduct preliminary exploratory data analysis aimed at diagnosing any major issues with an imported data set. pandas and pandas-profiling offers a clean and straightforward methodology to uncover issues such as data outliers, missing data, as well as summary statistical reports."
  },
  {
    "objectID": "intro.html#what-are-some-important-data-set-characteristics",
    "href": "intro.html#what-are-some-important-data-set-characteristics",
    "title": "Introduction",
    "section": "What are Some Important Data Set Characteristics?",
    "text": "What are Some Important Data Set Characteristics?\nThere are several characteristics that are arguably important, but we will only consider those covered in this workshop series. Let’s start with the fundamentals that will help guide us."
  },
  {
    "objectID": "intro.html#diagnostics",
    "href": "intro.html#diagnostics",
    "title": "Introduction",
    "section": "Diagnostics",
    "text": "Diagnostics\nWhen importing data sets, it is important to consider characteristics about the data columns, rows, and individual cells.\n\n\nVariables\nName of each variable\n\n\n   Pregnancies  Glucose  BloodPressure  ...  Age  Outcome  Age_group\n0            6      148             72  ...   50        1     Middle\n1            1       85             66  ...   31        0     Middle\n2            8      183             64  ...   32        1     Middle\n3            1       89             66  ...   21        0      Young\n4            0      137             40  ...   33        1     Middle\n\n[5 rows x 10 columns]\n\n\n\n\nTypes\nData type of each variable\n\n\nPregnancies                   int64\nGlucose                       int64\nBloodPressure                 int64\nSkinThickness                 int64\nInsulin                       int64\nBMI                         float64\nDiabetesPedigreeFunction    float64\nAge                           int64\nOutcome                       int64\nAge_group                    object\ndtype: object\n\n\n\nNumerical: Continuous\nMeasurable numbers that are fractional or decimal and cannot be counted (e.g., time, height, weight)\n\n\n\n\n\n\n\nNumerical: Discrete\nCountable whole numbers or integers (e.g., number of successes or failures)\n\n\n\n\n\n\n\n\nCategorical: Nominal\nLabeling variables without any order or quantitative value (e.g., hair color, nationality)\n\n\n\n\n\n\n\nCategorical: Ordinal\nWhere there is a hierarchical order along a scale (e.g., ranks, letter grades, age groups)\n\n\n\n\n\n\n\n\nMissing Values (NAs)\nCells, rows, or columns without data\n\nMissing percent: percentage of missing values * Unique count: number of unique values.\nUnique rate: rate of unique value - unique count / total number of observations.\n\n\n\n   Pregnancies  Glucose  BloodPressure  ...  Outcome  Age_group  Outcome1\n0          6.0    148.0           72.0  ...      1.0     Middle       Yes\n1          1.0     85.0           66.0  ...      0.0        NaN        No\n2          8.0    183.0           64.0  ...      1.0     Middle       Yes\n3          1.0     89.0           66.0  ...      0.0      Young       NaN\n4          0.0      NaN           40.0  ...      1.0     Middle       Yes\n\n[5 rows x 11 columns]"
  },
  {
    "objectID": "intro.html#summary-statistics",
    "href": "intro.html#summary-statistics",
    "title": "Introduction",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nAbove we described some properties of data. However, you will need to know some descriptive characteristics of your data before you can move forward. Enter, summary statistics.\nSummary statistics allow you to summarize large amounts of information about your data as quickly as possible.\n\nCentral Tendency\nMeasuring a central property of your data. Some examples you’ve probably heard of are:\n\nMean: Average value\nMedian: Middle value\nMode: Most common value\n\n\n\n\n\n\nNotice that all values of central tendency can be pretty similar in this figure.\n\n\n\n\n\nHowever, in this figure, all measures are different. This will be important when we discuss statistical dispersion in chapter 3.\n\n\n\n\n\nStatistical Dispersion\nMeasure of data variability, scatter, or spread. Some examples you may have heard of:\n\nStandard deviation (SD): The amount of variation that occurs in a set of values.\nInterquartile range (IQR): The difference between the 75th and 25th percentiles\nOutliers: A value outside of \\(1.5 * IQR\\)\n\n\n\n\n\n\n\n\nDistribution Shape\nMeasures of describing the shape of a distribution, usually compared to a normal distribution (bell-curve)\n\nSkewness: The symmetry of the distribution\nKurtosis: The tailedness of the distribution\n\n\n\n\n\n\n\n\nStatistical Dependence (Correlation)\nMeasure of causality between two random variables (statistically). Notably, we approximate causality with correlations (see correlation \\(\\neq\\) causation)\n\nNumerical values, but you can compare numericals across categories (see the first plot above)."
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html",
    "href": "DiagnosingLikeDataDoctor.html",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "",
    "text": "Exploring a novel data set and produce an HTML interactive reports"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#objectives",
    "href": "DiagnosingLikeDataDoctor.html#objectives",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Objectives",
    "text": "Objectives\n\nLoad and explore a data set with publication quality tables\nDiagnose outliers and missing values in a data set\nPrepare an HTML summary report showcasing properties of a data set"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#required-setup",
    "href": "DiagnosingLikeDataDoctor.html#required-setup",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Interactive HTML EDA report\nfrom pandas_profiling import ProfileReport\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#load-and-examine-a-data-set",
    "href": "DiagnosingLikeDataDoctor.html#load-and-examine-a-data-set",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\nLoad data and view\nExamine columns and data types\nDefine box plots\nDescribe meta data\n\nWe will be using open source data from UArizona researchers for Test, Trace, Treat (T3) efforts offers two clinical diagnostic tests (Antigen, RT-PCR) to determine whether an individual is currently infected with the COVID-19 virus. (Merchant et al. 2022)\n\n# Read csv \ndata = pd.read_csv(\"data/daily_summary.csv\")\n\n# What does the data look like\ndata.head()\n\n  result_date      affil_category  ... test_count          test_source\n0  2020-08-04            Employee  ...          5        Campus Health\n1  2020-08-04            Employee  ...          0        Campus Health\n2  2020-08-04            Employee  ...          1  Test All Test Smart\n3  2020-08-04            Employee  ...          0  Test All Test Smart\n4  2020-08-04  Off-Campus Student  ...          9        Campus Health\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#diagnose-your-data",
    "href": "DiagnosingLikeDataDoctor.html#diagnose-your-data",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndiagnose = data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9180 entries, 0 to 9179\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   result_date     9180 non-null   object\n 1   affil_category  9180 non-null   object\n 2   test_type       9180 non-null   object\n 3   test_result     9180 non-null   object\n 4   test_count      9180 non-null   int64 \n 5   test_source     9180 non-null   object\ndtypes: int64(1), object(5)\nmemory usage: 430.4+ KB\n\n\n\nColumn: name of each variable\nNon-Null Count: number of missing values\nDType: data type of each variable"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#summary-statistics-of-your-data",
    "href": "DiagnosingLikeDataDoctor.html#summary-statistics-of-your-data",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Summary Statistics of your Data",
    "text": "Summary Statistics of your Data\n\nNumerical Variables\n\n# Summary statistics of our numerical columns\ndata.describe()\n\n        test_count\ncount  9180.000000\nmean     46.771024\nstd     129.475844\nmin       0.000000\n25%       0.000000\n50%       2.000000\n75%      16.000000\nmax    1472.000000\n\n\n\ncount: number of observations\nmean: arithmetic mean (average value)\nstd: standard deviation\nmin: minimum value\n25%: 1/4 quartile, 25th percentile\n50%: median, 50th percentile\n75%: 3/4 quartile, 75th percentile\nmax: maximum value\n\n\n\n\nOutliers\nValues outside of \\(1.5 * IQR\\)\n\n\n\nImage Credit: CÉDRIC SCHERER\n\n\n\nThere are several numerical variables that have outliers above, let’s see what the data look like with and without them\n\nCreate a table with columns containing outliers\nPlot outliers in a box plot and histogram\n\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Define the 25th and 75th percentiles\n  q25, q75 = round((dataRed_i.quantile(q=0.25)), 3), round((dataRed_i.quantile(q=0.75)), 3)\n  \n  # Define the interquartile range from the 25th and 75th percentiles defined above\n  IQR = round((q75 - q25), 3)\n  \n  # Calculate the outlier cutoff \n  cut_off = IQR * 1.5\n  \n  # Define lower and upper cut-offs\n  lower, upper = round((q25 - cut_off), 3), round((q75 + cut_off), 3)\n  \n  # Print the values\n  print(' ')\n  \n  # For each value of i_col, print the 25th and 75th percentiles and IQR\n  print(i_col, 'q25=', q25, 'q75=', q75, 'IQR=', IQR)\n  \n  # Print the lower and upper cut-offs\n  print('lower, upper:', lower, upper)\n\n  # Count the number of outliers outside the (lower, upper) limits, print that value\n  print('Number of Outliers: ', dataRed_i[(dataRed_i < lower) | (dataRed_i > upper)].count())\n\n \ntest_count q25= 0.0 q75= 16.0 IQR= 16.0\nlower, upper: -24.0 40.0\nNumber of Outliers:  1721\n\n\n\nq25: 1/4 quartile, 25th percentile\nq75: 3/4 quartile, 75th percentile\nIQR: interquartile range (q75-q25)\nlower: lower limit of \\(1.5*IQR\\) used to calculate outliers\nupper: upper limit of \\(1.5*IQR\\) used to calculate outliers\n\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Select only numerical columns\ndataRedColsList = data.select_dtypes(include = np.number)\n\n# Melt data from wide-to-long format\ndata_melted = pd.melt(dataRedColsList)\n\n# Boxplot of all numerical variables\nsns.boxplot(data = data_melted, x = 'variable', y = 'value', hue = 'variable' , width = 0.20)\n\n\n\n\nNote the extreme number of outliers represented in the boxplot\n\n# Find Q1, Q3, and interquartile range (IQR) for each column\nQ1 = dataRedColsList.quantile(q = .25)\nQ3 = dataRedColsList.quantile(q = .75)\nIQR = dataRedColsList.apply(stats.iqr)\n\n# Only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\ndata_clean = dataRedColsList[~((dataRedColsList < (Q1 - 1.5 * IQR)) | (dataRedColsList > (Q3 + 1.5 * IQR))).any(axis = 1)]\n\n# Melt data from wide-to-long format\ndata_clean_melted =  pd.melt(data_clean)\n\n# Boxplot of all numerical variables, with outliers removed via the IQR cutoff criteria\nsns.boxplot(data = data_clean_melted, x = 'variable', y = 'value', hue = 'variable' , width = 0.20)\n\n\n\n\nBut the distribution changes dramatically when we remove outliers with the IQR method (see above). Interestingly, there are a new set of “outliers” which results from a new IQR being calculated.\n\n\nMissing Values (NAs)\n\nTable showing the extent of NAs in columns containing them\n\n\n# Copy of the data\ndataNA = data\n\n# Randomly add NAs to all columns replacing 10% of values\nfor col in dataNA.columns:\n    dataNA.loc[dataNA.sample(frac = 0.1).index, col] = np.nan\n\n# Sum of NAs in each column (should be the same, 10% of all)   \ndataNA.isnull().sum()\n\nresult_date       918\naffil_category    918\ntest_type         918\ntest_result       918\ntest_count        918\ntest_source       918\ndtype: int64\n\n\nBar plot showing all NA values in each column. Since we randomly produced a set amount above the numbers will all be the same.\n\n# Bar plot showing the number of NAs in each column\nmsno.bar(dataNA, figsize = (8, 8), fontsize = 10)\n\n\n\n\n\n\n\nCategorical Variables\n\n# Select only categorical columns (objects) and describe\ndata.describe(exclude = [np.number]) \n\n       result_date      affil_category  ... test_result          test_source\ncount         8262                8262  ...        8262                 8262\nunique         541                   4  ...           3                    2\ntop     2020-09-23  Off-Campus Student  ...    Positive  Test All Test Smart\nfreq            25                3009  ...        4128                 4585\n\n[4 rows x 5 columns]\n\n\n\ncount: number of values in the column\nunique: the number of unique categories\ntop: category with the most observations\nfreq: number of observations in the top category"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#produce-an-html-summary-of-a-data-set",
    "href": "DiagnosingLikeDataDoctor.html#produce-an-html-summary-of-a-data-set",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Produce an HTML Summary of a Data Set",
    "text": "Produce an HTML Summary of a Data Set\n\n# Producing a pandas-profiling report \n# profile = ProfileReport(data, title = \"Pandas Profiling Report\")\n\n# HTML output\n# profile.to_widgets()\n\n\n\n\n\nMerchant, Nirav C, Jim Davis, George H Franks, Chun Ly, Fernando Rios, Todd Wickizer, Gary D Windham, and Michelle Yung. 2022. “University of Arizona Test-Trace-Treat COVID-19 Testing Results.” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14869740.V3."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html",
    "href": "ExploringLikeDataAdventurer.html",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "",
    "text": "Exploring the normality of numerical columns in a novel data set"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#objectives",
    "href": "ExploringLikeDataAdventurer.html#objectives",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Objectives",
    "text": "Objectives\n\nUsing summary statistics to better understand individual columns in a data set.\nAssessing data normality in numerical columns.\nAssessing data normality within groups."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#required-setup",
    "href": "ExploringLikeDataAdventurer.html#required-setup",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Statistical modeling\nimport statsmodels.api as sm\n\n# increase font size of all seaborn plot elements\nsns.set(font_scale = 1.25)"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#load-and-examine-a-data-set",
    "href": "ExploringLikeDataAdventurer.html#load-and-examine-a-data-set",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\nWe will be using open source data from UArizona researchers that investigates the effects of climate change on canopy trees. (Meredith, Ladd, and Werner 2021)\n\n# Read csv \ndata = pd.read_csv(\"data/Data_Fig2_Repo.csv\")\n\n# What does the data look like\ndata.head()\n\n      Date                Group    Sap_Flow  TWaterFlux      pLWP      mLWP\n0  10/4/19  Drought-sens-canopy  184.040975   82.243292 -0.263378 -0.679769\n1  10/4/19   Drought-sens-under    2.475989    1.258050 -0.299669 -0.761326\n2  10/4/19   Drought-tol-canopy   10.598949    4.405479 -0.437556 -0.722557\n3  10/4/19    Drought-tol-under    4.399854    2.055276 -0.205224 -0.702858\n4  10/5/19  Drought-sens-canopy  182.905444   95.865255 -0.276928 -0.708261"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#diagnose-your-data",
    "href": "ExploringLikeDataAdventurer.html#diagnose-your-data",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndiagnose = data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 588 entries, 0 to 587\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Date        588 non-null    object \n 1   Group       588 non-null    object \n 2   Sap_Flow    480 non-null    float64\n 3   TWaterFlux  588 non-null    float64\n 4   pLWP        276 non-null    float64\n 5   mLWP        308 non-null    float64\ndtypes: float64(4), object(2)\nmemory usage: 27.7+ KB\n\n\n\nColumn: name of each variable\nNon-Null Count: number of missing values\nDType: data type of each variable\n\n\n\nBox Plot\n\n\n\nImage Credit: CÉDRIC SCHERER\n\n\n\n\n\nSkewness\n\n\n\n(c) Andrey Akinshin\n\n\n\n\nNOTE\n\n“Skewness” has multiple definitions. Several underlying equations mey be at play\nSkewness is “designed” for distributions with one peak (unimodal); it’s meaningless for distributions with multiple peaks (multimodal).\nMost default skewness definitions are not robust: a single outlier could completely distort the skewness value.\nWe can’t make conclusions about the locations of the mean and the median based on the skewness sign.\n\n\n\n\n\nKurtosis\n\n\n\n(c) Andrey Akinshin\n\n\n\nNOTE\n\nThere are multiple definitions of kurtosis - i.e., “kurtosis” and “excess kurtosis,” but there are other definitions of this measure.\nKurtosis may work fine for distributions with one peak (unimodal); it’s meaningless for distributions with multiple peaks (multimodal).\nThe classic definition of kurtosis is not robust: it could be easily spoiled by extreme outliers."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#describe-your-continuous-data",
    "href": "ExploringLikeDataAdventurer.html#describe-your-continuous-data",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Describe your Continuous Data",
    "text": "Describe your Continuous Data\n\n# Summary statistics of our numerical columns\ndata.describe()\n\n         Sap_Flow  TWaterFlux        pLWP        mLWP\ncount  480.000000  588.000000  276.000000  308.000000\nmean    25.091576   11.925722   -0.609055   -1.029703\nstd     40.520386   19.048809    0.227151    0.295834\nmin      0.172630    0.101381   -1.433333   -1.812151\n25%      2.454843    1.293764   -0.714008   -1.227326\n50%      5.815661    2.995357   -0.586201   -0.946656\n75%     16.371703    7.577102   -0.450000   -0.808571\nmax    184.040975   96.012719   -0.205224   -0.545165\n\n\n\ncount: number of observations\nmean: arithmetic mean (average value)\nstd: standard deviation\nmin: minimum value\n25%: 1/4 quartile, 25th percentile\n50%: median, 50th percentile\n75%: 3/4 quartile, 75th percentile\nmax: maximum value\n\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Define the 25th and 75th percentiles\n  q25, q75 = round((dataRed_i.quantile(q = 0.25)), 3), round((dataRed_i.quantile(q = 0.75)), 3)\n  \n  # Define the interquartile range from the 25th and 75th percentiles defined above\n  IQR = round((q75 - q25), 3)\n  \n  # Calculate the outlier cutoff \n  cut_off = IQR * 1.5\n  \n  # Define lower and upper cut-offs\n  lower, upper = round((q25 - cut_off), 3), round((q75 + cut_off), 3)\n  \n  # Skewness\n  skewness = round((dataRed_i.skew()), 3) \n  \n  # Kurtosis\n  kurtosis = round((dataRed_i.kurt()), 3)\n  \n  # Number of outliers\n  outliers = dataRed_i[(dataRed_i < lower) | (dataRed_i > upper)].count()\n  \n  # Print a blank row\n  print('')\n  \n  # Print the column name\n  print(i_col)\n  \n  # For each value of i_col, print the 25th and 75th percentiles and IQR\n  print('q25 =', q25, 'q75 =', q75, 'IQR =', IQR)\n  \n  # Print the lower and upper cut-offs\n  print('lower, upper:', lower, upper)\n  \n  # Print skewness and kurtosis\n  print('skewness =', skewness, 'kurtosis =', kurtosis)\n  \n  # Count the number of outliers outside the (lower, upper) limits, print that value\n  print('Number of Outliers: ', outliers)\n\n\nSap_Flow\nq25 = 2.455 q75 = 16.372 IQR = 13.917\nlower, upper: -18.42 37.248\nskewness = 2.153 kurtosis = 4.197\nNumber of Outliers:  116\n\nTWaterFlux\nq25 = 1.294 q75 = 7.577 IQR = 6.283\nlower, upper: -8.13 17.002\nskewness = 2.081 kurtosis = 3.884\nNumber of Outliers:  139\n\npLWP\nq25 = -0.714 q75 = -0.45 IQR = 0.264\nlower, upper: -1.11 -0.054\nskewness = -1.105 kurtosis = 1.767\nNumber of Outliers:  12\n\nmLWP\nq25 = -1.227 q75 = -0.809 IQR = 0.418\nlower, upper: -1.854 -0.182\nskewness = -0.797 kurtosis = -0.181\nNumber of Outliers:  0\n\n\n\nq25: 1/4 quartile, 25th percentile\nq75: 3/4 quartile, 75th percentile\nIQR: interquartile range (q75-q25)\nlower: lower limit of \\(1.5*IQR\\) used to calculate outliers\nupper: upper limit of \\(1.5*IQR\\) used to calculate outliers\nskewness: skewness\nkurtosis: kurtosis"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#describe-categorical-variables",
    "href": "ExploringLikeDataAdventurer.html#describe-categorical-variables",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Describe Categorical Variables",
    "text": "Describe Categorical Variables\n\n# Select only categorical columns (objects) \ndata.describe(exclude=[np.number]) \n\n           Date                Group\ncount       588                  588\nunique      147                    4\ntop     10/4/19  Drought-sens-canopy\nfreq          4                  147\n\n\n\n\nGroup Descriptive Statistics\n\n# Grouped describe by one column, stacked \nGroups = data.groupby('Group').describe().unstack(1)\n\n# Print all rows\nprint(Groups.to_string())\n\n                   Group              \nSap_Flow    count  Drought-sens-canopy    120.000000\n                   Drought-sens-under     120.000000\n                   Drought-tol-canopy     120.000000\n                   Drought-tol-under      120.000000\n            mean   Drought-sens-canopy     85.269653\n                   Drought-sens-under       1.448825\n                   Drought-tol-canopy       9.074309\n                   Drought-tol-under        4.573516\n            std    Drought-sens-canopy     41.313962\n                   Drought-sens-under       0.803858\n                   Drought-tol-canopy       1.395670\n                   Drought-tol-under        0.902430\n            min    Drought-sens-canopy     33.370450\n                   Drought-sens-under       0.172630\n                   Drought-tol-canopy       5.904610\n                   Drought-tol-under        2.171780\n            25%    Drought-sens-canopy     53.975162\n                   Drought-sens-under       0.534165\n                   Drought-tol-canopy       8.119410\n                   Drought-tol-under        4.053346\n            50%    Drought-sens-canopy     76.717782\n                   Drought-sens-under       1.665492\n                   Drought-tol-canopy       9.286552\n                   Drought-tol-under        4.944842\n            75%    Drought-sens-canopy     94.068107\n                   Drought-sens-under       2.194299\n                   Drought-tol-canopy      10.404117\n                   Drought-tol-under        5.139685\n            max    Drought-sens-canopy    184.040975\n                   Drought-sens-under       2.475989\n                   Drought-tol-canopy      10.705455\n                   Drought-tol-under        5.726712\nTWaterFlux  count  Drought-sens-canopy    147.000000\n                   Drought-sens-under     147.000000\n                   Drought-tol-canopy     147.000000\n                   Drought-tol-under      147.000000\n            mean   Drought-sens-canopy     40.404061\n                   Drought-sens-under       0.751770\n                   Drought-tol-canopy       4.357234\n                   Drought-tol-under        2.189824\n            std    Drought-sens-canopy     19.027997\n                   Drought-sens-under       0.429073\n                   Drought-tol-canopy       0.940353\n                   Drought-tol-under        0.597511\n            min    Drought-sens-canopy     12.377738\n                   Drought-sens-under       0.101381\n                   Drought-tol-canopy       2.036843\n                   Drought-tol-under        0.953906\n            25%    Drought-sens-canopy     25.220908\n                   Drought-sens-under       0.274190\n                   Drought-tol-canopy       3.601341\n                   Drought-tol-under        1.735003\n            50%    Drought-sens-canopy     38.630891\n                   Drought-sens-under       0.824875\n                   Drought-tol-canopy       4.460778\n                   Drought-tol-under        2.198131\n            75%    Drought-sens-canopy     50.096197\n                   Drought-sens-under       1.112890\n                   Drought-tol-canopy       5.112844\n                   Drought-tol-under        2.686605\n            max    Drought-sens-canopy     96.012719\n                   Drought-sens-under       1.801823\n                   Drought-tol-canopy       5.976890\n                   Drought-tol-under        3.654336\npLWP        count  Drought-sens-canopy     69.000000\n                   Drought-sens-under      69.000000\n                   Drought-tol-canopy      69.000000\n                   Drought-tol-under       69.000000\n            mean   Drought-sens-canopy     -0.669932\n                   Drought-sens-under      -0.696138\n                   Drought-tol-canopy      -0.629909\n                   Drought-tol-under       -0.440243\n            std    Drought-sens-canopy      0.246390\n                   Drought-sens-under       0.283935\n                   Drought-tol-canopy       0.095571\n                   Drought-tol-under        0.131879\n            min    Drought-sens-canopy     -1.299263\n                   Drought-sens-under      -1.433333\n                   Drought-tol-canopy      -0.863656\n                   Drought-tol-under       -0.746667\n            25%    Drought-sens-canopy     -0.790573\n                   Drought-sens-under      -0.800000\n                   Drought-tol-canopy      -0.706479\n                   Drought-tol-under       -0.520487\n            50%    Drought-sens-canopy     -0.705942\n                   Drought-sens-under      -0.592118\n                   Drought-tol-canopy      -0.602841\n                   Drought-tol-under       -0.406439\n            75%    Drought-sens-canopy     -0.473290\n                   Drought-sens-under      -0.521217\n                   Drought-tol-canopy      -0.571356\n                   Drought-tol-under       -0.360789\n            max    Drought-sens-canopy     -0.263378\n                   Drought-sens-under      -0.299669\n                   Drought-tol-canopy      -0.437556\n                   Drought-tol-under       -0.205224\nmLWP        count  Drought-sens-canopy     77.000000\n                   Drought-sens-under      77.000000\n                   Drought-tol-canopy      77.000000\n                   Drought-tol-under       77.000000\n            mean   Drought-sens-canopy     -1.319148\n                   Drought-sens-under      -1.097537\n                   Drought-tol-canopy      -0.892554\n                   Drought-tol-under       -0.809572\n            std    Drought-sens-canopy      0.298107\n                   Drought-sens-under       0.263522\n                   Drought-tol-canopy       0.091729\n                   Drought-tol-under        0.170603\n            min    Drought-sens-canopy     -1.812151\n                   Drought-sens-under      -1.808333\n                   Drought-tol-canopy      -1.073619\n                   Drought-tol-under       -1.168716\n            25%    Drought-sens-canopy     -1.525563\n                   Drought-sens-under      -1.335521\n                   Drought-tol-canopy      -0.945841\n                   Drought-tol-under       -0.907041\n            50%    Drought-sens-canopy     -1.354771\n                   Drought-sens-under      -1.054159\n                   Drought-tol-canopy      -0.890061\n                   Drought-tol-under       -0.735647\n            75%    Drought-sens-canopy     -1.111942\n                   Drought-sens-under      -0.907564\n                   Drought-tol-canopy      -0.828777\n                   Drought-tol-under       -0.699087\n            max    Drought-sens-canopy     -0.679769\n                   Drought-sens-under      -0.546152\n                   Drought-tol-canopy      -0.707789\n                   Drought-tol-under       -0.545165"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#sec-testing-normality",
    "href": "ExploringLikeDataAdventurer.html#sec-testing-normality",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Testing Normality",
    "text": "Testing Normality\n\nShapiro-Wilk test & Q-Q plots\nTesting overall normality of two columns\nTesting normality of groups\n\n\n\nNormality of Columns\n\n\nShapiro-Wilk Test\nShapiro-Wilk test looks at whether a target distribution is sample form a normal distribution\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Specify desired column\ni_col = dataCopyFin.Sap_Flow\n\n# Normality test\nstat, p = stats.shapiro(i_col)\n\nprint('')\nprint('Shapiro-Wilk Test for Normality')\n\nShapiro-Wilk Test for Normality\n\nprint('')\nprint('Sap_Flow')\n\nSap_Flow\n\nprint('Statistic = %.3f, p = %.3f' % (stat, p))\n  \n# Interpret\n\nStatistic = 0.603, p = 0.000\n\nalpha = 0.05\n  \nif p > alpha:\n  print('Sample looks Gaussian (fail to reject H0)')\nelse:\n  print('Sample does not look Gaussian (reject H0)')\n\nSample does not look Gaussian (reject H0)\n\n\nYou can also run the Shapiro-Wilk test on all numerical columns with a for-loop\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Select only numerical columns\ndataRed = dataCopyFin.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Normality test\n  stat, p = stats.shapiro(dataRed_i)\n  \n  # Print a blank, the column name, the statistic and p-value\n  print('')\n  print(i_col)\n  print('Statistic = %.3f, p = %.3f' % (stat, p))\n  \n  # Interpret\n  alpha = 0.05\n  \n  # Print the interpretation\n  if p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\n  else:\n      print('Sample does not look Gaussian (reject H0)')\n\n\nSap_Flow\nStatistic = 0.603, p = 0.000\nSample does not look Gaussian (reject H0)\n\nTWaterFlux\nStatistic = 0.600, p = 0.000\nSample does not look Gaussian (reject H0)\n\npLWP\nStatistic = 0.929, p = 0.000\nSample does not look Gaussian (reject H0)\n\nmLWP\nStatistic = 0.940, p = 0.000\nSample does not look Gaussian (reject H0)\n\n\n\n\n\nQ-Q Plots\nPlots of the quartiles of a target data set and plot it against predicted quartiles from a normal distribution.\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Specify desired column\ni_col = dataCopyFin.Sap_Flow\n\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Sap_Flow Density plot')\n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Sap_Flow Q-Q plot')\nplt.tight_layout()\nplt.show()\n\n\n\n\nYou can also produce these plots for all numerical columns with a for-loop.\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Select only numerical columns\ndataRed = dataCopyFin.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \n\n# Overall figure that subplots fill\nfig, axes = plt.subplots(ncols = 2, nrows = 4, sharex = True, figsize = (4, 4))\n\n# Fill the subplots\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    # Subplots\n    fig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n    \n    # Density plot\n    sns.kdeplot(dataRed[k], linewidth = 5, ax = ax1)\n    ax1.set_title(f'{k} Density Plot')\n    \n    # Q-Q plot\n    sm.qqplot(dataRed[k], line='s', ax = ax2)\n    ax2.set_title(f'{k} QQ Plot')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\nNormality within Groups\nLooking within Age_group at the subgroup normality.\n\nShapiro-Wilk Test\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Pivot the data from long-to-wide with pivot, using Date as the index, so that a column is created for each Group and numerical column subset\ndataPivot = dataCopyFin.pivot(index = 'Date', columns = 'Group', values = ['Sap_Flow', 'TWaterFlux', 'pLWP', 'mLWP'])\n\n# Select only numerical columns\ndataRed = dataPivot.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # normality test\n  stat, p = stats.shapiro(dataRed_i)\n  \n  print('')\n  print(i_col)\n  print('Statistics = %.3f, p = %.3f' % (stat, p))\n  \n  # interpret\n  alpha = 0.05\n  \n  if p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\n  else:\n      print('Sample does not look Gaussian (reject H0)')\n\n\n('Sap_Flow', 'Drought-sens-canopy')\nStatistics = 0.869, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('Sap_Flow', 'Drought-sens-under')\nStatistics = 0.889, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('Sap_Flow', 'Drought-tol-canopy')\nStatistics = 0.950, p = 0.008\nSample does not look Gaussian (reject H0)\n\n('Sap_Flow', 'Drought-tol-under')\nStatistics = 0.908, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('TWaterFlux', 'Drought-sens-canopy')\nStatistics = 0.885, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('TWaterFlux', 'Drought-sens-under')\nStatistics = 0.856, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('TWaterFlux', 'Drought-tol-canopy')\nStatistics = 0.973, p = 0.147\nSample looks Gaussian (fail to reject H0)\n\n('TWaterFlux', 'Drought-tol-under')\nStatistics = 0.977, p = 0.233\nSample looks Gaussian (fail to reject H0)\n\n('pLWP', 'Drought-sens-canopy')\nStatistics = 0.969, p = 0.086\nSample looks Gaussian (fail to reject H0)\n\n('pLWP', 'Drought-sens-under')\nStatistics = 0.867, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('pLWP', 'Drought-tol-canopy')\nStatistics = 0.952, p = 0.010\nSample does not look Gaussian (reject H0)\n\n('pLWP', 'Drought-tol-under')\nStatistics = 0.964, p = 0.044\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-sens-canopy')\nStatistics = 0.962, p = 0.034\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-sens-under')\nStatistics = 0.956, p = 0.016\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-tol-canopy')\nStatistics = 0.962, p = 0.034\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-tol-under')\nStatistics = 0.852, p = 0.000\nSample does not look Gaussian (reject H0)\n\n\n\n\n\nQ-Q Plots\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Pivot the data from long-to-wide with pivot, using Date as the index, so that a column is created for each Group and numerical column subset\ndataPivot = dataCopyFin.pivot(index = 'Date', columns = 'Group', values = ['Sap_Flow', 'TWaterFlux', 'pLWP', 'mLWP'])\n\n# Select only numerical columns\ndataRed = dataPivot.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \nfig, axes = plt.subplots(ncols = 2, nrows = 8, sharex = True, figsize = (2 * 4, 8 * 4))\n\n# Generate figures for all numerical grouped data subsets\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    sm.qqplot(dataRed[k], line = 's', ax = ax)\n    ax.set_title(f'{k}\\n QQ Plot')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMeredith, Laura, S. Nemiah Ladd, and Christiane Werner. 2021. “Data for \"Ecosystem Fluxes During Drought and Recovery in an Experimental Forest\".” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14632593.V1."
  }
]